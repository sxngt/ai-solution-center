# <%= className %> Service

<%= description %>

## ğŸ“‹ Service Information

- **Service Name**: <%= className %>
- **Author**: <%= author %>
- **Primary LLM Provider**: <%= llmProvider %>
- **Port**: <%= port %>
- **Version**: 1.0.0

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+
- PostgreSQL 15+
- Redis 7+
- (Optional) Ollama for local LLM support

### Installation

1. Install dependencies:
```bash
yarn install
```

2. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. Start the database:
```bash
yarn docker:up
```

4. Run the service:
```bash
yarn nx serve <%= propertyName %>
```

The service will be available at `http://localhost:<%= port %>/api`

## ğŸ”§ Environment Variables

```env
# Service Configuration
PORT=<%= port %>

# Database
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=developer
DATABASE_PASSWORD=dev123
DATABASE_NAME=ai_solution

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# LLM Providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
OLLAMA_BASE_URL=http://localhost:11434

# Authentication
JWT_SECRET=your-secret-key
JWT_EXPIRES_IN=1h
JWT_REFRESH_EXPIRES_IN=7d

# CORS
CORS_ORIGIN=*
```

## ğŸ“š API Endpoints

### Public Endpoints

- `GET /api` - Service information
- `GET /api/health` - Health check
- `GET /api/providers` - Available LLM providers

### Protected Endpoints (Require Authentication)

- `POST /api/chat` - Generate AI response

#### Chat Request Example:

```json
POST /api/chat
Authorization: Bearer <access_token>
Content-Type: application/json

{
  "message": "Hello, how can you help me?",
  "model": "gpt-4o-mini"
}
```

#### Chat Response Example:

```json
{
  "response": "Hello! I'm an AI assistant...",
  "provider": "<%= llmProvider %>",
  "usage": {
    "promptTokens": 45,
    "completionTokens": 67,
    "totalTokens": 112
  },
  "timestamp": "2024-01-15T10:30:00.000Z",
  "user": "user-id"
}
```

## ğŸ§ª Testing

```bash
# Unit tests
yarn nx test <%= propertyName %>

# E2E tests
yarn nx e2e <%= propertyName %>

# Test coverage
yarn nx test <%= propertyName %> --coverage
```

## ğŸ³ Docker

Build and run with Docker:

```bash
yarn nx build <%= propertyName %>
docker build -t <%= propertyName %>-service .
docker run -p <%= port %>:<%= port %> <%= propertyName %>-service
```

## ğŸ›  Development Commands

```bash
# Start development server
yarn nx serve <%= propertyName %>

# Build for production
yarn nx build <%= propertyName %>

# Lint code
yarn nx lint <%= propertyName %>

# Format code
yarn nx format <%= propertyName %>
```

## ğŸ— Architecture

This service is built using:

- **Framework**: NestJS
- **Database**: PostgreSQL with TypeORM
- **Authentication**: JWT with Passport
- **LLM Integration**: Multi-provider support (OpenAI, Claude, Ollama)
- **Validation**: class-validator
- **Logging**: Winston
- **Testing**: Jest

## ğŸ”’ Authentication

This service uses JWT-based authentication. To access protected endpoints:

1. Register a user account
2. Login to get access token
3. Include token in Authorization header: `Bearer <token>`

## ğŸ¤– LLM Integration

The service supports multiple LLM providers:

- **<%= llmProvider.charAt(0).toUpperCase() + llmProvider.slice(1) %>** (Primary)
- **Ollama** (Fallback/Local)
- **Other providers** (Configurable)

Provider fallback is automatic when the primary provider is unavailable.

## ğŸ“Š Monitoring

- Health endpoint: `GET /api/health`
- Provider status: `GET /api/providers`
- Application logs via Winston

## ğŸ¤ Contributing

1. Create a feature branch
2. Make your changes
3. Add tests
4. Run linting and tests
5. Submit a pull request

## ğŸ“ License

This service is part of the AI Solution Center project.

---

Generated by AI Solution Center Service Generator